# -*- coding: utf-8 -*-
"""LLR Sim1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15dkp-U1dfg5Lv9A84Y7gbr3hrMEdXOOq
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/'My Drive'/llrdata

#All imports
from numpy import genfromtxt
import numpy as np
from keras import models
from keras.layers import Dense, Dropout, BatchNormalization
from keras.utils import to_categorical
from keras.datasets import mnist
from keras.utils.vis_utils import model_to_dot
from IPython.display import SVG
from keras.backend import sigmoid
from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation
from keras.layers import Add, Dense, Input, UpSampling1D, LeakyReLU
from keras.models import Sequential,  Model
Xtrain = genfromtxt('Xtrain_256.csv', delimiter=',')
Ytrain = genfromtxt('Ytrain_256.csv', delimiter=',')

import numpy as np
 
def llr_256_approx(data, sigma):
    llr = []
    i = data[0]
    q = data[1]
    p = 1
    alphascale = 16
    scaling = (4 / np.sqrt(170/p) ) * (alphascale/(2*sigma**2))
    llr.append(scaling * i) #1
    llr.append(scaling * q) #2
    llr.append(scaling * (-abs(i) + 8/np.sqrt(170/p))) #3
    llr.append(scaling * (-abs(q) + 8/np.sqrt(170/p))) #4
    llr.append(scaling * (-abs(abs(i) - 8/np.sqrt(170/p)) + 4/np.sqrt(170/p))) #5
    llr.append(scaling * (-abs(abs(q)-8/np.sqrt(170/p))+4/np.sqrt(170/p))) #6
    llr.append(scaling * (-abs(abs(abs(i)-8/np.sqrt(170/p))-4/np.sqrt(170/p))+2/np.sqrt(170/p))) #7
    llr.append(scaling * (-abs(abs(abs(q)-8/np.sqrt(170/p))-4/np.sqrt(170/p))+2/np.sqrt(170/p))) #8
    return np.array(llr)
 
def generateDL(max_r, min_r, sigma):
    Xtrain = []
    Ytrain = []
    for i in range(10000): #no. of data points can be changed acc to model
        i = np.random.uniform(min_r, max_r) #generates real part decimal number in between max_r and min_r
        q = np.random.uniform(min_r, max_r) #generates imaginary part decimal number in between max_r and min_r
        Xtrain.append([i, q])
        Ytrain.append(llr_256_approx([i, q], sigma))
    Xtrain = np.array(Xtrain)
    Ytrain = np.array(Ytrain)
    return Xtrain, Ytrain
# all three variables can be changed acc to AM 
sigma = 1
max_r = 5
min_r = -5
##
 
Xtrain, Ytrain = generateDL(max_r, min_r, sigma)

#Defining the activation function
def swish(x, beta = 1):
    return (x * sigmoid(beta * x))
get_custom_objects().update({'swish': Activation(swish)})

idx_random = np.random.rand(len(Xtrain)) < (1/9)  # used to seperate data into training and validation sets
train_data, train_label = Xtrain[~idx_random,:] , Ytrain[~idx_random,:]
Xval, Yval = Xtrain[idx_random,:] , Ytrain[idx_random,:]

output_length= Ytrain.shape[1] #need to change the ouput length of the neural nertwork  

# input tensor for a 3-channel 256x256 image
x = Input(shape=(2,))
# 3x3 conv with 3 output channels (same as input channels)
y = Dense(8)(x)
#y1= BatchNormalization()(y)
z= Activation(activation="swish")(y)
#t = Add()([y, z])
f= Dense(output_length)(z)
#a= UpSampling1D(size=2)(y)
#a1= Activation(activation="swish")(a)

#out= Add()([f,y])
model = Model(input = x, output = f)

model.compile(optimizer='adam',
              loss='mse',
              metrics=['mse'])
 
# Train model
model.fit(train_data, train_label,
          batch_size=32,
          epochs=150,
          verbose=1,
          validation_data=(Xval, Yval))

#original_weights = model.get_weights()

def create_sparsity_masks(sparsity):
    weights_list = model.get_weights()
    masks = []
    for weights in weights_list:
        #We can ignore biases
        if len(weights.shape) > 1:
            weights_abs = np.abs(weights)
            masks.append((weights_abs>np.percentile(weights_abs,sparsity))*1.)
    return masks

from keras.constraints import Constraint
from keras import backend as K
class Sparse(Constraint):
    '''
    We will use one variable: Mask
    After we train our model dense model,
    we will save the weights and analyze them.
    We will create a mask where 1 means the
    number is far away enough from 0 and 0
    if it is to close to 0. We will multiply
    the weights by 0(making them 0) if they
    are supposed to be masked.
    '''
    
    def __init__(self, mask):
        self.mask = K.cast_to_floatx(mask)
    
    def __call__(self,x):
        return self.mask * x
    
    def get_config(self):
        return {'mask': self.mask}

masks = create_sparsity_masks(50)#Closest 30% to 0

def make_model():
  x = Input(shape=(2,))
  # 3x3 conv with 3 output channels (same as input channels)
  y = Dense(8, kernel_constraint=Sparse(masks[0]))(x)
  #y1= BatchNormalization()(y)
  z= Activation(activation="swish")(y)
  #t = Add()([y, z])
  f= Dense(output_length, kernel_constraint=Sparse(masks[1]))(z)
  #out= Add()([f,y])
  model = Model(input = x, output = f)
  return model

#if u want to run anything above this cell then restart the runtime

epochs= 150
for i in range(10,60,10):
  print("Removed", i, "percentage of connections")
  masks = create_sparsity_masks(i)
  model= make_model()
  model.compile(optimizer='adam',
                loss='mse',
                metrics=['mse'])
  
  # Train model
  x = model.fit(train_data, train_label,
            batch_size=32,
            epochs=epochs,
            verbose=1,
            validation_data=(Xval, Yval))

Ytrain[1:11]

model.predict(Xtrain[1:11])

Xtrain[2]

Ytrain[2]